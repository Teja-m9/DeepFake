# -*- coding: utf-8 -*-
"""Deep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xHuYdrtqPLVTUTlUOjlnpaLXThF4cKs8
"""

pip install kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle

!mv "kaggle (1).json" ~/.kaggle/kaggle.json

!chmod 600 ~/.kaggle/kaggle.json

!!kaggle competitions download -c deepfake-detection-challenge

!unzip deepfake-detection-challenge.zip

!pip install efficientnet_pytorch
import os
import json
import cv2
import numpy as np
from tqdm import tqdm
from glob import glob
from PIL import Image

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from efficientnet_pytorch import EfficientNet

def extract_frames(video_path, frame_rate=1):
    cap = cv2.VideoCapture(video_path)
    frames = []
    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps == 0: return frames
    frame_interval = int(fps * frame_rate)
    frame_count = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if frame_count % frame_interval == 0:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(Image.fromarray(frame))
        frame_count += 1
    cap.release()
    return frames

import pandas as pd

# Path to train data folder
train_video_dir = "/content/train_sample_videos"
metadata_path = os.path.join(train_video_dir, "metadata.json")

# Load labels
with open(metadata_path, 'r') as f:
    metadata = json.load(f)

video_paths = []
labels = []

for video_name, info in metadata.items():
    video_path = os.path.join(train_video_dir, video_name)
    if os.path.exists(video_path):
        label = 0 if info['label'] == 'REAL' else 1
        video_paths.append(video_path)
        labels.append(label)

class DeepFakeDataset(Dataset):
    def __init__(self, video_paths, labels, transform=None, max_frames=5):
        self.video_paths = video_paths
        self.labels = labels
        self.transform = transform
        self.max_frames = max_frames

    def __len__(self):
        return len(self.video_paths)

    def __getitem__(self, idx):
        video_path = self.video_paths[idx]
        label = self.labels[idx]
        frames = extract_frames(video_path)

        if len(frames) > self.max_frames:
            frames = frames[:self.max_frames]

        if self.transform:
            frames = [self.transform(frame) for frame in frames]

        if len(frames) == 0:  # fallback if video is unreadable
            frames = [torch.zeros(3, 224, 224) for _ in range(self.max_frames)]

        return torch.stack(frames), label

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

dataset = DeepFakeDataset(video_paths, labels, transform)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)

class DeepFakeModel(nn.Module):
    def __init__(self):
        super(DeepFakeModel, self).__init__()
        self.base_model = EfficientNet.from_pretrained('efficientnet-b0')
        self.classifier = nn.Linear(1000, 1)

    def forward(self, x):
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)
        feats = self.base_model(x)
        feats = self.classifier(feats)
        feats = feats.view(B, T)
        return torch.mean(feats, dim=1)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = DeepFakeModel().to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

for epoch in range(3):
    model.train()
    running_loss = 0.0

    for frames, labels in tqdm(dataloader):
        frames = frames.to(device)
        labels = labels.float().to(device)

        outputs = model(frames)
        loss = criterion(outputs.squeeze(), labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {running_loss / len(dataloader)}")

def predict_video(model, video_path, transform, max_frames=5):
    model.eval()
    frames = extract_frames(video_path)
    frames = frames[:max_frames]

    if len(frames) == 0:
        return 0.5  # fallback neutral confidence

    frames = [transform(f) for f in frames]
    frames = torch.stack(frames).unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(frames)
        prob = torch.sigmoid(output).item()
        return prob

test_video_path = ""  # replace with a real file
prob = predict_video(model, test_video_path, transform)
print(f"Prediction Confidence (Fake = 1): {prob:.4f}")
print("Prediction:", "FAKE" if prob > 0.5 else "REAL")

torch.save(model, "deepfake_detector_full.pth")

import tensorflow as tf
import cv2
import os
import random
import numpy as np

class DeepfakeDataset(tf.keras.utils.Sequence):
    def __init__(self, video_paths, labels, frames_per_clip=16, image_size=128, batch_size=4, shuffle=True):
        self.video_paths = video_paths
        self.labels = labels
        self.frames_per_clip = frames_per_clip
        self.image_size = image_size
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        return int(np.ceil(len(self.video_paths) / self.batch_size))

    def on_epoch_end(self):
        self.indexes = np.arange(len(self.video_paths))
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __getitem__(self, index):
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        video_batch = [self.video_paths[k] for k in indexes]
        labels_batch = [self.labels[k] for k in indexes]

        X, y = self.__data_generation(video_batch, labels_batch)
        return X, y

    def __data_generation(self, video_batch, labels_batch):
        X = np.zeros((len(video_batch), self.frames_per_clip, self.image_size, self.image_size, 3), dtype=np.float32)
        y = np.array(labels_batch, dtype=np.float32)

        for i, video_path in enumerate(video_batch):
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            frame_indices = sorted(random.sample(range(total_frames), min(self.frames_per_clip, total_frames)))

            frames = []
            for j in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, j)
                success, frame = cap.read()
                if success:
                    frame = cv2.resize(frame, (self.image_size, self.image_size))
                    frame = frame / 255.0  # Normalize
                    frames.append(frame)
            cap.release()
            while len(frames) < self.frames_per_clip:
                frames.append(np.zeros((self.image_size, self.image_size, 3)))

            X[i] = np.array(frames)
        return X, y

from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import TimeDistributed, LSTM, Dense, Input, GlobalAveragePooling2D
from tensorflow.keras.models import Model

def create_deepfake_model(frames_per_clip=16, image_size=128):
    base_cnn = MobileNetV2(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))
    base_cnn.trainable = False  # Freeze for small datasets!

    input_layer = Input(shape=(frames_per_clip, image_size, image_size, 3))
    x = TimeDistributed(base_cnn)(input_layer)
    x = TimeDistributed(GlobalAveragePooling2D())(x)

    x = LSTM(128, return_sequences=False)(x)
    output = Dense(1, activation='sigmoid')(x)

    model = Model(inputs=input_layer, outputs=output)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Example paths & labels
video_paths = ["./data/train_sample_videos/video1.mp4", "./data/train_sample_videos/video2.mp4"]
labels = [0, 1]  # 0 = real, 1 = fake

# Dataset instance
dataset = DeepfakeDataset(video_paths, labels, batch_size=2)

# Create the model
model = create_deepfake_model()

# Train!
model.fit(dataset, epochs=5)

def prepare_single_video(video_path, frames_per_clip=16, image_size=128):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # Handle short videos
    frame_indices = sorted(np.random.choice(
        range(total_frames),
        frames_per_clip,
        replace=total_frames < frames_per_clip
    ))

    frames = []
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        success, frame = cap.read()
        if success:
            frame = cv2.resize(frame, (image_size, image_size))
            frame = frame / 255.0  # Normalize
            frames.append(frame)
    cap.release()

    # Pad if not enough frames
    while len(frames) < frames_per_clip:
        frames.append(np.zeros((image_size, image_size, 3)))

    video_tensor = np.array(frames, dtype=np.float32)
    video_tensor = np.expand_dims(video_tensor, axis=0)  # Batch dimension
    return video_tensor

# Path to your test video
my_video = '/content/fake1.mp4'

# Prepare the video using the same function
input_tensor = prepare_single_video(my_video)

# Predict directly without saving/loading
prediction = model.predict(input_tensor)[0][0]

# Interpret result
if prediction > 0.5:
    print(f"Prediction: FAKE ({prediction:.4f})")
else:
    print(f"Prediction: REAL ({prediction:.4f})")

